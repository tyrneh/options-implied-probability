# PRD: SVIâ€‘JW Calibration (practice + code)

**TL;DR:** Build a fast, **SVIâ€‘JW** (Stochastic Volatility Inspired â€“ *Jumpâ€‘Wings*) calibration service that fits each maturity slice robustly, enforces **no static arbitrage** within slices and across maturities, and exposes a clean API with wellâ€‘chosen bounds, penalties, and reproducible defaults.

---

## 1) McKinseyâ€‘style topâ€‘down plan

- **What weâ€™re solving.** We convert noisy option quotes into an **arbitrageâ€‘free volatility surface** using the **SVIâ€‘JW** parameterization for interpretability (ATM level/skew and wing slopes), then publish parameters and pricing functions. **Why it matters:** fewer hedging pathologies and stable extrapolation.
- **How weâ€™ll do it.**
  1. Clean data â†’ forward moneyness â†’ implied vols/total variance.
  2. **Per maturity**, calibrate **JW** parameters under tight bounds; penalize butterfly arbitrage and outâ€‘ofâ€‘spread fits; polish with a deterministic method.
  3. **Across maturities**, enforce termâ€‘structure consistency using **SSVI** conditions or callâ€‘price interpolation to remove calendar arbitrage.
- **What weâ€™ll deliver.** A Python library returning JW and rawâ€‘SVI parameters, diagnostics, and pricers, with SLAs on speed and stability.
- **Design choice.** **JW** is more interpretable; **raw SVI** remains common for slice fitting; **SSVI** anchors the surface for noâ€‘arbitrage across maturities.

---

## 2) Detailed spec and steps

### A. Scope & users
- **Users.** Volatility traders, risk managers, and model engineers.
- **Inâ€‘scope.** Equity/Index vanillas; multiple expiries; outlierâ€‘robust calibration; static noâ€‘arbitrage checks.
- **Outâ€‘ofâ€‘scope (v1).** Smile dynamics, localâ€‘vol extraction, and exotics pricing.

### B. Inputs and preprocessing
- **Inputs.** Timeâ€‘stamped quotes (bid/ask/mid, size), strikes, expiries, spot, dividends, and interest rates.
- **Preprocessing.** Compute forwards per expiry; logâ€‘moneyness \(x=\ln(K/F)\); implied vol \(\sigma\); **total variance \(w=\sigma^2\,\tau\)**. Filter stale/locked markets; downâ€‘weight unreliable tails.

### C. Perâ€‘maturity model and bounds
- **Model.** Use **SVIâ€‘JW** parameters \((v_\tau,\psi_\tau,p_\tau,c_\tau,\hat v_\tau)\) with the standard mapping to raw SVI to evaluate \(w(x)\). JW parameters correspond to **ATM variance**, **ATM skew**, **left/right wing slopes**, and **minimum variance**.
- **Guardrails (rawâ€‘SVI domain).** Enforce: \(\sigma>0\), \(b\ge 0\), \(\rho\in[-1,1]\), reasonable bounds on \(m\) from the strike range, and small positive lower bounds \(a,\sigma\) to avoid degeneracy. Cap \(b\) to prevent pathological wings.

### D. Objective function (per slice)
- **Loss.** Weighted squared error in **implied vol** or **total variance** versus market mid.
- **Penalties.**
  - **Butterfly arbitrage penalty:** penalize when Durrlemanâ€™s convexity density test fails (i.e., negative density).
  - **Bidâ€‘ask envelope penalty:** penalize if fitted vols leave quoted spreads.
  - **Regularization:** soft priors linking neighboring expiries for stability.

### E. Optimization
- **Strategy.** Run a global search (e.g., Differential Evolution) followed by a **polish** step (e.g., Lâ€‘BFGS or Nelderâ€‘Mead). Enable multiâ€‘start; warmâ€‘start from neighbor expiries.
- **Speedâ€‘up (QE split).** Use a **Quasiâ€‘Explicit split** that treats \((\sigma,m)\) as outer variables and solves an inner leastâ€‘squares for the remaining parameters each iteration; map back to raw/JW. This materially reduces runtime.

### F. Acrossâ€‘maturity surface (no calendar arbitrage)
- **Preferred:** Fit slices first, then **interpolate call prices** monotonically in maturity to guarantee no calendar arbitrage.
- **Alternative:** Calibrate a global **SSVI** (Surface SVI) with conditions on \(\theta(\tau)\) and \(\phi(\theta)\) that are necessary/sufficient to avoid butterfly **and** calendar arbitrage; then read off induced JWâ€‘like quantities.

### G. Data weighting & robustness
- Weight points by **vega** and **volume**; downâ€‘weight illiquid tails; cap extreme residuals. Use expiryâ€‘specific outlier filters to stabilize shortâ€‘dated wings.

### H. API & outputs
- **Endpoints.**
  - `POST /calibrate`: input = quotes by expiry; output = perâ€‘expiry **JW** and **raw SVI** params, fit error, and arbitrage flags.
  - `GET /surface`: return smooth samplers \(w(\tau,x)\) and \(\sigma(\tau,x)\).
  - `POST /price`: price via Blackâ€“Scholes using fitted \(\sigma\).
- **Artifacts.** JSON parameters, PNG plots, and CSV dumps for auditability.

### I. Acceptance criteria
- **Withinâ€‘slice:** Nonâ€‘negative density on a grid; â‰¥99% of liquid points inside bidâ€“ask; RMSE below a preset threshold.
- **Across maturities:** Monotone ATM total variance; no calendar arbitrage under priceâ€‘space checks or SSVI conditions.
- **Ops:** 95thâ€‘percentile latency < 200 ms per slice on ~200 points; reproducible to 1eâ€‘6 with the same seed.

### J. Risks & mitigations
- **Noisy wings / shortâ€‘dated jumps.** Use wing penalties and vega/volume weighting.
- **Parameter degeneracy.** Guard with bounds and the QE split; monitor condition numbers.
- **Surface stitching.** Prefer priceâ€‘space interpolation or SSVI constraints to avoid calendar arbitrage.

---

## 3) Notes on SVIâ€‘JW vs other variations (practice reality)

- **Does SVIâ€‘JW have only upsides?** Not entirely. **Upsides:** parameters map to traderâ€‘intuitive quantities (ATM level/skew and wing slopes) and are helpful for constraints. **Tradeâ€‘offs:** you still must enforce noâ€‘arbitrage; JW adds a mapping step to/from raw SVI; numerical edge cases appear near parameter boundaries.
- **What is most used in practice?** **Raw SVI** is still the workhorse for **perâ€‘expiry** fitting; **JW** is used for interpretation and stabilizing constraints; **SSVI** or priceâ€‘space interpolation is used to ensure a **calendarâ€‘free surface** across maturities.

---

## 4) Implementation checklist (engineerâ€™s toâ€‘do)

1. Data pipeline: forwards, moneyness, implied vols, total variance.
2. Bounds + QE split; mappings between JW and raw SVI.
3. Objective with butterfly and bidâ€“ask penalties; globalâ€‘thenâ€‘local solver.
4. Slice diagnostics; export JW/raw; pricing helper.
5. Surface stitching (priceâ€‘space or SSVI); calendarâ€‘arbitrage checks.
6. Tests: regression, stress tests on wings/short tenors; performance budget.

---

## Glossary

- **SVI:** *Stochastic Volatility Inspired*; a fiveâ€‘parameter totalâ€‘variance smile model.
- **JW:** *Jumpâ€‘Wings*; an SVI reâ€‘parameterization using ATM level/skew and wing slopes.
- **ATM / OTM:** *Atâ€‘ / Outâ€‘ofâ€‘theâ€‘Money*; strike near / far from the forward price.
- **SSVI:** *Surface SVI*; an SVI family with termâ€‘structure conditions that prevent static arbitrage.
- **Butterfly arbitrage:** A withinâ€‘slice convexity violation implying negative riskâ€‘neutral density.

---

## References (practical guides)

- Aurell, *SVI â€“ Calibration and Arbitrage Considerations* (MSc thesis). https://www.diva-portal.org/smash/get/diva2:744907/FULLTEXT02.pdf
- Quant Chronicles, *Model Calibration Using Optimization Engines â€“ An Example Using Implied Volatility*. https://medium.com/quant-chronicles/model-calibration-using-optimization-engines-an-example-using-implied-volatility-cf6c3233a8ba



##  2. SVI Calibration Code Review and Refactor Plan

2.1 Executive Summary
The current package correctly: cleans quotes, infers a forward via putâ€“call parity, computes implied volatilities (IV), and calibrates a raw SVI slice with sensible bounds and two noâ€‘arbitrage penalties (butterfly via g(k)g(k)g(k) and callâ€‘spread monotonicity). This is implemented mainly in estimator.py (data flow & orchestration), surface_fitting.py (dispatch to â€œsviâ€ vs â€œbsplineâ€), and svi.py (parameters, objective, Lâ€‘BFGSâ€‘B optimizer). However, it does not yet implement SVIâ€‘JW, uses only a local optimizer (no global search), lacks bidâ€“ask envelope penalties and vega/volume weighting, and exposes limited diagnostics. The refactor below adds JW as a firstâ€‘class parametrization, cleanly separates preprocessing/fitting/diagnostics, and introduces a robust global + polish calibration path.

2.2 Technical Plan
ðŸ§© Part 1: Code vs PRD Alignment
Perâ€‘slice calibration flow
	â€¢ âœ… Forward & tenor: You infer a forward via apply_put_call_parity(..) and pass it into SVI fitting (surface_fitting._fit_svi(...)), then price on a strike grid and derive the PDF/CDF (Breedenâ€“Litzenberger).
	â€¢ âš ï¸ Singleâ€‘expiry only: No crossâ€‘maturity stitching or SSVIâ€‘style (Surface SVI) constraints, which is fine for RND v1 but out of scope for the PRDâ€™s â€œcalendarâ€‘free surfaceâ€ stretch goal.
Objective, constraints, penalties (in svi.py)
	â€¢ âœ… Objective in total variance: Leastâ€‘squares on w(k)=Ïƒ2Ï„w(k)=\sigma^2\tauw(k)=Ïƒ2Ï„ is used.
	â€¢ âœ… Arbitrage checks: You include a butterfly penalty via g_function(k) and a callâ€‘spread monotonicity penalty.
	â€¢ âš ï¸ No bidâ€“ask envelope penalty: The PRD requires a hinge penalty if the fitted IV leaves [bid_iv,ask_iv][{\rm bid\_iv}, {\rm ask\_iv}][bid_iv,ask_iv]. Currently the bid/ask IVs are computed (for plotting) but not enforced in the loss.
	â€¢ âš ï¸ No data weighting: Residuals are unweighted. The PRD calls for vega/volume weighting and downâ€‘weighting wings.
	â€¢ âš ï¸ Static bounds are coarse: Bounds (_build_bounds) are mostly fixed caps; the PRD recommends dataâ€‘adaptive bounds (e.g., wider mmm when strikes span is narrow, lower Ïƒminâ¡\sigma_{\min}Ïƒminâ€‹ safeguards) and guardrails linked to tenor.
Optimization strategy
	â€¢ âš ï¸ Local only: You use Lâ€‘BFGSâ€‘B from a heuristic _initial_guess(...). The PRD specifies global search (e.g., Differential Evolution or CMAâ€‘ES) then polish (Lâ€‘BFGSâ€‘B/Nelderâ€“Mead), with optional warmâ€‘starts from neighboring expiries.
Parametrizations
	â€¢ âš ï¸ Raw SVI only: SVIParameters(a,b,Ï,m,Ïƒ) are supported; SVIâ€‘JW is not implemented. The PRD requires JW for interpretability (ATM level/skew, wing slopes) and explicit JWâ†”raw mappings.
Diagnostics & outputs
	â€¢ âœ… You attach evaluation grids to the fitted curve and expose plotting.
	â€¢ âš ï¸ Missing structured diagnostics: Min g(k)g(k)g(k), fraction of points inside bidâ€“ask, RMSE metrics, and a summary of constraint activity are not exposed as a standard â€œfit report.â€


ðŸ›  Part 2: Refactor & Implementation Advice
A. Make SVIâ€‘JW a firstâ€‘class citizen
	1. Param classes

@dataclass(frozen=True)
class SVIParameters:
    a: float; b: float; rho: float; m: float; sigma: float

@dataclass(frozen=True)
class SVIJW:
    v: float      # ATM total variance w(0)
    psi: float    # ATM skew w'(0)
    p: float      # left wing slope magnitude
    c: float      # right wing slope magnitude
    vmin: float   # minimum total variance
	2. Deterministic mappings (rawâ†’JW always analytic).

def raw_to_jw(raw: SVIParameters) -> SVIJW:
    a,b,rho,m,sigma = raw.a,raw.b,raw.rho,raw.m,raw.sigma
    s = (m*m + sigma*sigma) ** 0.5
    v    = a + b*(s - rho*m)                  # ATM level
    psi  = b*(rho - m/s)                      # ATM skew
    p    = b*(1 - rho)                        # left wing slope (>0)
    c    = b*(1 + rho)                        # right wing slope (>0)
    vmin = a + b*sigma*(1 - rho*rho) ** 0.5   # minimum variance
    return SVIJW(v, psi, p, c, vmin)
	3. Numerical mapping (JWâ†’raw).
Compute b=p+c2b=\tfrac{p+c}{2}b=2p+câ€‹, Ï=câˆ’pp+c\rho=\tfrac{c-p}{p+c}Ï=p+ccâˆ’pâ€‹, then solve for s=m2+Ïƒ2s=\sqrt{m^2+\sigma^2}s=m2+Ïƒ2â€‹ from the two ATM identities w(0)=vw(0)=vw(0)=v and wâ€²(0)=Ïˆw'(0)=\psiwâ€²(0)=Ïˆ plus vminâ¡v_{\min}vminâ€‹ (use a 1â€‘D root for sss; recover m=s(Ïâˆ’Ïˆ/b)m=s(\rho - \psi/b)m=s(Ïâˆ’Ïˆ/b), Ïƒ=s1âˆ’(Ïâˆ’Ïˆ/b)2\sigma=s\sqrt{1-(\rho-\psi/b)^2}Ïƒ=s1âˆ’(Ïâˆ’Ïˆ/b)2â€‹, a=vminâ¡âˆ’bâ€‰Ïƒ1âˆ’Ï2a=v_{\min}-b\,\sigma\sqrt{1-\rho^2}a=vminâ€‹âˆ’bÏƒ1âˆ’Ï2â€‹). Verify numerically that raw_to_jw(jw_to_raw(.)) is identity to 1eâ€‘8.
	4. Add 'svi-jw' as an additional option to surface_method
B. Split optimization paths cleanly
	â€¢ Internal representation: Run the optimizer in raw space only (evaluation is easiest), but accept either raw or JW inputs/outputs. When the user requests JW, map the initial guess JWâ†’raw, optimize raw, then return both raw and JW in the result.
	â€¢ Global + polish:
		â—‹ Global: scipy.optimize.differential_evolution over _build_bounds(k, Ï„) to get a robust seed.
		â—‹ Polish: L-BFGS-B (with analytic gradients optional) or Nelderâ€“Mead for noisy data.
		â—‹ Determinism: Allow random_seed and max_iter in surface_options.
C. Strengthen the objective
	â€¢ Weighted residuals: Minimize âˆ‘iwi[wmodel(ki)âˆ’wi]2\sum_i w_i [w_{\text{model}}(k_i)-w_i]^2âˆ‘iâ€‹wiâ€‹[wmodelâ€‹(kiâ€‹)âˆ’wiâ€‹]2 with wiâˆw_i \proptowiâ€‹âˆ vega Ã— volume (fallback to vega if volume absent). Cap weights in deep wings; optionally Huberize residuals.
	â€¢ Bidâ€“ask envelope penalty: Hinge penalty when fitted IV leaves [bid_iv,ask_iv][{\rm bid\_iv},{\rm ask\_iv}][bid_iv,ask_iv].

def bid_ask_penalty(model_iv, bid_iv, ask_iv, lam=1e3):
    below = np.maximum(0.0, bid_iv - model_iv)
    above = np.maximum(0.0, model_iv - ask_iv)
    return lam * float(np.sum(below*below + above*above))
	â€¢ Keep existing: g(k) butterfly penalty and callâ€‘spread penalty; expose their weights in options.
D. Improve bounds & initial guesses
	â€¢ Dataâ€‘adaptive bounds:
		â—‹ mâˆˆ[kminâ¡âˆ’Î”,â€‰kmaxâ¡+Î”]m \in [k_{\min}-\Delta,\, k_{\max}+\Delta]mâˆˆ[kminâ€‹âˆ’Î”,kmaxâ€‹+Î”] with Î”=maxâ¡(1,kmaxâ¡âˆ’kminâ¡)\Delta=\max(1, k_{\max}-k_{\min})Î”=max(1,kmaxâ€‹âˆ’kminâ€‹).
		â—‹ Ïƒâ‰¥Ïƒminâ¡\sigma \ge \sigma_{\min}Ïƒâ‰¥Ïƒminâ€‹ (e.g., 1e-41\text{e-}41e-4), âˆ£Ïâˆ£â‰¤0.999|\rho| \le 0.999âˆ£Ïâˆ£â‰¤0.999.
		â—‹ Set bbb upper bound wider for short maturities; optionally relate to tenor.
	â€¢ Initial guess: from observed w(k)w(k)w(k): m0=argâ¡minâ¡wm_0=\arg\min wm0â€‹=argminw, a0=w(m0)a_0=w(m_0)a0â€‹=w(m0â€‹), small b0b_0b0â€‹, Ï0=0\rho_0=0Ï0â€‹=0, Ïƒ0â‰ˆ\sigma_0\approxÏƒ0â€‹â‰ˆ stdev of kkk. When JW is requested, seed via JWâ†’raw mapping from a heuristic JW guess.
E. Modularity & readability
	â€¢ Preprocessing module (prep.py): Keep parity, staleness filtering, price selection. Add a â€œrobust strike filterâ€ (remove obvious outliers, optional).
	â€¢ Fitting module (svi.py):
		â—‹ svi_options(...) â†’ replace dicts with a small @dataclass (typeâ€‘safe).
		â—‹ Expose calibrate_slice(...) -> FitResult returning: params (raw & JW), RMSE (weighted), min g(k)g(k)g(k), envelope breaches %, and solver diagnostics.
	â€¢ Surface dispatch (surface_fitting.py):
		â—‹ fit_surface(method="svi", options=...) now reads svi_parametrization, global_solver, polish_solver.
		â—‹ Returned callable attaches .params_raw, .params_jw, .diagnostics, and .grid.
	â€¢ Diagnostics: Add result.iv_smile() to include bid/ask IVs, weights, residuals, g(k)g(k)g(k) on a grid, and flags for any arbitrage violation.
F. Tests & examples
	â€¢ Roundâ€‘trip tests for JWâ†”raw mappings.
	â€¢ Noise stress tests (shortâ€‘dated wings), regression tests on a few expiries.
	â€¢ Timing tests to keep p95 < 200 ms/slice at ~200 points (with global+polish, cache the global seed per expiry).


## 3) SVI Calibration Failure Analysis (why it fails and how to harden it)
3.1 Whatâ€™s different vs the literature (and why this bites)
	â€¢ Butterfly test g(k) is wrong. The code computes
g = (1 âˆ’ kÂ·w'/(2w)) * (1 âˆ’ w'/2) âˆ’ (w'^2)*(1/(4w)+1/16) + 0.5Â·w'',
but the canonical condition is
g = (1 âˆ’ kÂ·w'/(2w))^2 âˆ’ (w'^2)/4Â·(1/w + 1/4) + 0.5Â·w''.
The missing square causes spurious negatives of g(k) and â€œfailureâ€ even when the slice is fine. Imperial College London
	â€¢ Local optimizer only. Calibration uses Lâ€‘BFGSâ€‘B with a single heuristic seed and wide, mostly static bounds; no global search or multiâ€‘start is attempted. This is brittle for skewed, shortâ€‘dated, or sparse smiles. 
	â€¢ Loss is unweighted and lacks market guards. Residuals are plain least squares in total variance; there is no vega/volume weighting and no bidâ€“ask envelope penalty, so deepâ€‘wing noise can dominate and push the fit into arbitrage. 
	â€¢ Auxiliary penalties/knobs. The callâ€‘spread penalty uses a fixed step 0.05 in logâ€‘moneyness, which is too coarse for short tenors and too fine for long ones; penalty weights are large and static, making the objective illâ€‘conditioned on some names. 
	â€¢ Fallback hides root causes. On failure you silently switch to Bâ€‘spline and proceed, so the observed â€œmany tickers fail SVIâ€ accumulates without targeted fixes. 
Why it matters: users see success, but the model is not the one intended in the PRD; diagnostics do not reveal why SVI failed. The public docs also emphasize a Bâ€‘spline smile by default, reinforcing the mismatch. 

3.2 Concrete fixes (code) and the tests to prove them
A) Fix the butterfly diagnostic now
Replace your g_function with the literatureâ€‘correct form (Gatheralâ€“Jacquier).

def g_function(k, params):
    k  = np.asarray(k, float)
    w  = svi_total_variance(k, params)
    wp = svi_first_derivative(k, params)
    wpp = svi_second_derivative(k, params)
    with np.errstate(divide="ignore", invalid="ignore"):
        g = (1.0 - (k*wp)/(2.0*w))**2 - 0.25*(wp**2)*(1.0/w + 0.25) + 0.5*wpp
    return np.where(np.isfinite(g), g, -np.inf)
Test: For known arbitrageâ€‘free SVI/SSVI parameter sets from the literature, assert min g(k) â‰¥ 0 on a dense grid; for obviously bad sets, assert min g(k) < 0. Imperial College London
B) Add a robust optimizer pipeline
	â€¢ Global + polish: Wrap the current objective in scipy.optimize.differential_evolution (or CMAâ€‘ES) to get a good seed, then polish with Lâ€‘BFGSâ€‘B. Keep random seeds and iteration budgets in options.
	â€¢ Multiâ€‘start (cheap mode): If global is disabled, run 5â€“10 randomized seeds (jitter m, Ïƒ, Ï) and keep the best.
Tests: On a daily SPY slice and 3â€“5 idiosyncratic tickers, record success rate and RMSE before/after; require â‰¥95% success and â‰¥30% RMSE reduction vs current. (Use the repoâ€™s existing I/O & plotting for visual regression.) 
C) Weight the data and respect the tape
	â€¢ Vega/volume weights: Weight squared residuals by vega Ã— volume (clip extremes; fallback to vega).
	â€¢ Bidâ€“ask envelope: Add a hinge penalty if model IV exits [bidIV, askIV]; expose weight in options.
Tests:
	1. On liquid underlyings, require â‰¥99% of points inside the envelope.
	2. On lowâ€‘volume names, confirm wings do not dominate: compare weighted vs unweighted RMSE on the central 50% of strikes (weighted should be lower). 
D) Make penalties adaptive
	â€¢ Callâ€‘spread step: Use step = 0.5 * median(diff(k)) rather than a constant 0.05; scale weight down for very short maturities.
	â€¢ Bounds: Keep Ïƒ â‰¥ 1eâˆ’4, |Ï| â‰¤ 0.999 but set m âˆˆ [k_min âˆ’ span, k_max + span] with span = max(1, k_max âˆ’ k_min), and cap b with a tenorâ€‘aware upper bound (shorter expiries need tighter b_max).
Tests: Parameterâ€‘stress suite that sweeps step, weights, and bounds over grids; assert stability (no explosions, consistent minima).
E) Improve the initial guess
Initialize m at the empirical min of total variance; set a = w_min, Ï=0, bâ‰ˆ0.1, Ïƒâ‰ˆstd(k) (already close), then run a QE split (outer (m,Ïƒ)(m,Ïƒ)(m,Ïƒ), inner linear LS for the remaining three) to get a sharper seed before the global stage. Test: With the global stage off, QEâ€‘seeded local runs should beat current RMSE â‰¥20% on average.
F) Observability
Expose a standard fit report: RMSE (weighted/unweighted), min g(k), % inside bidâ€“ask, active constraints, and optimizer diagnostics; log when the fallback to Bâ€‘spline is used. This turns â€œSVI failed a lotâ€ into measurable categories. 

3.3 Minimal patch set you can ship first
	1. Fix g_function (oneâ€‘line square);
	2. Add bidâ€“ask envelope penalty and adaptive callâ€‘spread step;
	3. Enable multiâ€‘start (5 seeds) before Lâ€‘BFGSâ€‘B;
	4. Weight by vega (volume optional).
These four changes typically take SVI success from â€œspottyâ€ to â€œrobustâ€ even before you add a full global optimizer.
